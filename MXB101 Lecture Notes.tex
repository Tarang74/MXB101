%!TEX program = xelatex
\documentclass{article}
\usepackage{LaTeX-Submodule/template}

% Additional packages & macros
\usepackage{multicol}

% Header and footer
\newcommand{\unitName}{Probability and Stochastic Modelling 1}
\newcommand{\unitTime}{Semester 1, 2022}
\newcommand{\unitCoordinator}{Dr Alexander Browning}
\newcommand{\documentAuthors}{\textsc{Tarang Janawalkar}}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Events and Probability}
\subsection{Events}
\begin{definition}[Event]
    An event is a set of outcomes in a random experiment commonly denoted by a capital letter.
    Events can be simple (a single event) or compound (two or more simple events).
\end{definition}
\begin{definition}[Sample space]
    The set of all possible outcomes of an experiment is known as the sample space
    for that experiment and is denoted \(\Omega\).
\end{definition}
\begin{definition}[Intersection]
    An intersection between two events \(A\) and \(B\) describes the set of outcomes that occur in both \(A\) and \(B\).
    The intersection can be represented using the set {\ttfamily{AND}} operator (\(\cap\)) --- \(A \cap B\) (or \(AB\)).
\end{definition}
\begin{definition}[Disjoint]
    Disjoint (mutually exclusive) events are two events that cannot occur simultaneously, or have no common outcomes.
\end{definition}
\begin{theorem}[Intersection of disjoint events]
    The intersection of disjoint events results in the null set (\(\varnothing\)).
\end{theorem}
\begin{lemma}
    Disjoint events are \textbf{dependent} events as the occurrence of one means the other cannot occur.
\end{lemma}
\begin{definition}[Union]
    A union of two events \(A\) and \(B\) describes the set of outcomes in either \(A\) or \(B\).
    The union is represented using the set {\ttfamily{OR}} operator (\(\cup\)) --- \(A \cup B\).
\end{definition}
\begin{definition}[Complement]
    The complement of an event \(E\) is the set of all other outcomes in \(\Omega\).
    The complement of \(E\) is denoted \(\overline{E}\).
\end{definition}
\begin{theorem}[Intersection of complement set]
    \begin{equation*}
        A\overline{A} = \varnothing
    \end{equation*}
\end{theorem}
\begin{theorem}[Union of complement set]
    \begin{equation*}
        A \cup \overline{A} = \Omega
    \end{equation*}
\end{theorem}
\begin{definition}[Subset]
    \(A\) is a (non-strict) subset of \(B\) if all elements in \(A\) are also in \(B\).
    This can be denoted as \(A \subset B\).
\end{definition}
\begin{theorem}
    All events \(E\) are subsets of \(\Omega\).
\end{theorem}
\begin{theorem}
    Given \(A \subset B\)
    \begin{equation*}
        AB = A \quad\quad \text{and} \quad\quad A \cup B = B
    \end{equation*}
\end{theorem}
\begin{corollary}
    Given \(\varnothing \subset E\)
    \begin{equation*}
        \varnothing E = \varnothing \quad\quad \text{and} \quad\quad \varnothing \cup E = E
    \end{equation*}
\end{corollary}
\begin{theorem}[Associative Identities]
    \begin{align*}
        A \left( BC \right)            & = \left( AB \right) C            \\
        A \cup \left( B \cup C \right) & = \left( A \cup B \right) \cup C
    \end{align*}
\end{theorem}
\begin{theorem}[Distributive Identities]
    \begin{align*}
        A \left(B \cup C\right) & = AB \cup AC                                      \\
        A \cup BC               & = \left( A \cup B \right) \left( A \cup C \right)
    \end{align*}
\end{theorem}
\subsection{Probability}
\begin{definition}[Probability]
    Probability is a measure of the likeliness of an event occurring. The probability of
    an event \(E\) is denoted \(\Pr{\left( E \right)}\) (sometimes \(\mathrm{P}\left( E \right)\)).
    \begin{equation*}
        0 \le \Pr{\left( E \right)} \le 1
    \end{equation*}
    where a probability of 0 never happens, and 1 always happens.
\end{definition}
\begin{theorem}[Probability of \(\Omega\)]
    \begin{equation*}
        \Pr{\left( \Omega \right)} = 1
    \end{equation*}
\end{theorem}
\begin{theorem}[Complement rule]
    The probability of the complement of \(E\) is given by
    \begin{equation*}
        \Pr{\left( \overline{E} \right)} = 1 - \Pr{\left( E \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Multiplication rule for independent events]
    The probability of the intersection between two independent events \(A\) and \(B\) is given by
    \begin{equation*}
        \Pr{\left( AB \right)} = \Pr{\left( A \right)} \Pr{\left( B \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Addition rule for independent events]
    The probability of the union between two independent events \(A\) and \(B\) is given by
    \begin{equation*}
        \Pr{\left( A \cup B \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)} - \Pr{\left( AB \right)}.
    \end{equation*}
    If \(A\) and \(B\) are disjoint, then \(\Pr{\left( AB \right)} = 0\), so that \(\Pr{\left( A \cup B \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)}\).
\end{theorem}
\begin{corollary}[Addition rule for 3 events]
    The addition rule for 3 events is as follows
    \begin{equation*}
        \Pr{\left( A \cup B \cup C \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)} + \Pr{\left( C \right)} - \Pr{\left( AB \right)} - \Pr{\left( AC \right)} - \Pr{\left( BC \right)} + \Pr{\left( ABC \right)}.
    \end{equation*}
\end{corollary}
\begin{proof}
    If we write \(D = A \cup B\) and apply the addition rule twice, we have
    \begin{align*}
        \Pr{\left( A \cup B \cup C \right)} & = \Pr{\left( D \cup C \right)}                                                                                                                                                               \\
                                            & = \Pr{\left( D \right)} + \Pr{\left( C \right)} - \Pr{\left( DC \right)}                                                                                                                     \\
                                            & = \Pr{\left( A \cup B \right)} + \Pr{\left( C \right)} - \Pr{\left( \left( A \cup B \right)C \right)}                                                                                        \\
                                            & = \Pr{\left( A \right)} + \Pr{\left( B \right)} - \Pr{\left( AB \right)} + \Pr{\left( C \right)} - \Pr{\left( AC \cup BC \right)}                                                            \\
                                            & = \Pr{\left( A \right)} + \Pr{\left( B \right)} - \Pr{\left( AB \right)} + \Pr{\left( C \right)} - \left( \Pr{\left( AC \right)} + \Pr{\left( BC \right)} - \Pr{\left( ACBC \right)} \right) \\
                                            & = \Pr{\left( A \right)} + \Pr{\left( B \right)} + \Pr{\left( C \right)} - \Pr{\left( AB \right)} - \Pr{\left( AC \right)} - \Pr{\left( BC \right)} + \Pr{\left( ABC \right)}
    \end{align*}
\end{proof}
\begin{theorem}[De Morgan's laws]
    Recall De Morgan's Laws:
    \begin{align*}
        \overline{A \cup B} & = \overline{A} \ \overline{B}     \\
        \overline{AB}       & = \overline{A} \cup \overline{B}.
    \end{align*}
    Taking the negation of both sides and applying the complement rule yields
    \begin{align*}
        \Pr{\left( A \cup B \right)} & = 1 - \Pr{\left( \overline{A} \ \overline{B} \right)}    \\
        \Pr{\left( AB \right)}       & = 1 - \Pr{\left( \overline{A} \cup \overline{B} \right)}
    \end{align*}
\end{theorem}
\subsection{Circuits}
A signal can pass through a circuit if there is a functional path from start to finish.

We can define a circuit where each component \(i\) functions with probability \(p\),
and is independent of other components.

Then \(W_i\) to be the event in which the associated component \(i\) functions, we can
determine the event \(S\) in which the system functions,
and probability \(\Pr{\left( S \right)}\) that the system functions.

As the probability that any component functions is \(p\), in other words
\begin{equation*}
    \Pr{\left( W_i \right)} = p,
\end{equation*}
\(\Pr{\left( S \right)}\) will be a function of \(p\) defined \(f:\left[ 0,\; 1 \right] \to \left[ 0,\; 1 \right]\).
\section{Independence}
\begin{definition}[Conditional probability]
    When discussing multiple events, it is possible that the occurrence of one event changes
    the probability that another will occur. This can be denoted using a vertical bar,
    and is read as ``the probability of event \(A\) given \(B\)'':
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} = \frac{\Pr{\left( A B \right)}}{\Pr{\left( B \right)}}.
    \end{equation*}
\end{definition}
\begin{definition}[Multiplication rule]
    For events \(A\) and \(B\), the general multiplication rule states that
    \begin{equation*}
        \Pr{\left( A B \right)} = \Pr{\left( A \,\vert\, B \right)} \Pr{\left( B \right)}
    \end{equation*}
\end{definition}
\begin{theorem}[Independent events]
    If \(A\) and \(B\) are independent events then
    \begin{align*}
        \Pr{\left( A \,\vert\, B \right)} & = \Pr{\left( A \right)} \\
        \Pr{\left( B \,\vert\, A \right)} & = \Pr{\left( B \right)}
    \end{align*}
\end{theorem}
\begin{theorem}[Complement of independent events]
    If \(A\) and \(B\) are independent, all complement pairs are also independent.
    Given
    \begin{align*}
        \Pr{\left( A \,\vert\, B \right)} & = \Pr{\left( A \right)} \\
        \Pr{\left( B \,\vert\, A \right)} & = \Pr{\left( B \right)}
    \end{align*}
    the following statements are also true
    \begin{align*}
        \Pr{\left( A \,\vert\, \overline{B} \right)}            & = \Pr{\left( A \right)}            & \Pr{\left( B \,\vert\, \overline{A} \right)}            & = \Pr{\left( B \right)}            \\
        \Pr{\left( \overline{A} \,\vert\, B \right)}            & = \Pr{\left( \overline{A} \right)} & \Pr{\left( \overline{B} \,\vert\, A \right)}            & = \Pr{\left( \overline{B} \right)} \\
        \Pr{\left( \overline{A} \,\vert\, \overline{B} \right)} & = \Pr{\left( \overline{A} \right)} & \Pr{\left( \overline{B} \,\vert\, \overline{A} \right)} & = \Pr{\left( \overline{B} \right)}
    \end{align*}
\end{theorem}
\subsection{Probability Rules with Conditional}
ALl probability rules hold when conditioning on some event \(C\).
\begin{theorem}[Complement rule with condition]
    \begin{equation*}
        \Pr{\left( \overline{A} \,\vert\, C \right)} = 1 - \Pr{\left( A \,\vert\, C \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Addition rule with condition]
    \begin{equation*}
        \Pr{\left( A \cup B \,\vert\, C \right)} = \Pr{\left( A \,\vert\, C \right)} + \Pr{\left( B \,\vert\, C \right)} - \Pr{\left( AB \,\vert\, C \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Multiplication rule with condition]
    \begin{equation*}
        \Pr{\left( A B \,\vert\, C \right)} = \Pr{\left( A \,\vert\, BC \right)} \Pr{\left( B \,\vert\, C \right)}
    \end{equation*}
\end{theorem}
In the above examples, all probabilities are conditional on the sample space, hence we are effectively
changing the sample space.
\subsection{Conditional Independence}
\begin{definition}[Conditional independence]
    Suppose events \(A\) and \(B\) are not independent, i.e.,
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} \neq \Pr{\left( A \right)}
    \end{equation*}
    but they become independent when conditioned with another event \(C\), i.e.,
    \begin{equation*}
        \Pr{\left( A \,\vert\, BC \right)} = \Pr{\left( A \,\vert\, C \right)}
    \end{equation*}
    Here we say that \(A\) and \(B\) are \textbf{conditionally independent} given \(C\). Furthermore
    \begin{equation*}
        \Pr{\left( AB \,\vert\, C \right)} = \Pr{\left( A \,\vert\, C \right)} \Pr{\left( B \,\vert\, C \right)}
    \end{equation*}
    Conversely, events \(A\) and \(B\) may be conditionally dependent but unconditionally independent, i.e.,
    \begin{align*}
        \Pr{\left( A \,\vert\, B \right)}  & = \Pr{\left( A \right)}                                                \\
        \Pr{\left( A \,\vert\, BC \right)} & \neq \Pr{\left( A \,\vert\, C \right)}                                 \\
        \Pr{\left( AB \,\vert\, C \right)} & = \Pr{\left( A \,\vert\, BC \right)} \Pr{\left( B \,\vert\, C \right)}
    \end{align*}
\end{definition}
\begin{theorem}
    Given events \(A\), \(B\), and \(C\). Pairwise independence does not imply mutual independence. I.e.,
    \begin{equation*}
        \begin{cases}
            \Pr{\left( A B \right)} = \Pr{\left( A \right)} \Pr{\left( B \right)} \\
            \Pr{\left( A C \right)} = \Pr{\left( A \right)} \Pr{\left( C \right)} \\
            \Pr{\left( B C \right)} = \Pr{\left( B \right)} \Pr{\left( C \right)}
        \end{cases}
    \end{equation*}
    does not imply
    \begin{equation*}
        \Pr{\left( A B C \right)} = \Pr{\left( A \right)} \Pr{\left( B \right)} \Pr{\left( C \right)}.
    \end{equation*}
\end{theorem}
In summary, independence should not be assumed unless explicitly stated.
\subsection{Disjoint Events}
\begin{theorem}[Probability of disjoint events]
    The probability of disjoint events \(A\) and \(B\) is given by
    \begin{align*}
        \Pr{\left( AB \right)}          & = 0  \\
        \Pr{\left( \varnothing \right)} & = 0.
    \end{align*}
    Disjoint events are highly dependent events, since the occurrence of one means the other cannot occur.
    This implies
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} = 0
    \end{equation*}
\end{theorem}
\subsection{Subsets}
\begin{theorem}[Probability of subsets]
    If \(A \subset B\) then \(\Pr{\left( A \right)} \le \Pr{\left( B \right)}\).
    We also know that \(\Pr{\left( AB \right)} = \Pr{\left( A \right)}\) and \(\Pr{\left( A \cup B \right)} = \Pr{\left( B \right)}\).

    Here, if \(A\) happens, then \(B\) definitely happens.
    \begin{equation*}
        \Pr{\left( B \,\vert\, A \right)} = 1
    \end{equation*}
    Given \(\Pr{\left( AB \right)} = \Pr{\left( A \right)}\)
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} = \frac{\Pr{\left( A \right)}}{\Pr{\left( B \right)}}
    \end{equation*}
    These events are also highly dependent.
\end{theorem}
\section{Total Probability}
\begin{definition}[Marginal probability]
    Marginal probability is the probability of an event \linebreak irrespective of the outcome of another variable.
\end{definition}
\begin{theorem}[Total probability for complements]
    By writing the event \(A\) as \(AB \cup A\overline{B}\), and noting that \(AB\) and \(A\overline{B}\) are disjoint,
    the marginal probability of \(A\) is given by
    \begin{equation*}
        \Pr{\left( A \right)} = \Pr{\left( AB \right)} + \Pr{\left( A\overline{B} \right)}.
    \end{equation*}
    By applying the multiplication rule to each joint probability:
    \begin{equation*}
        \Pr{\left( A \right)} = \Pr{\left( A \,\vert\, B \right)}\Pr{\left( B \right)} + \Pr{\left( A \,\vert\, \overline{B} \right)}\Pr{\left( \overline{B} \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Law of total probability]
    The previous theorem partitioned \(\Omega\) into disjoint events \(B\) and \(\overline{B}\).

    By partitioning \(\Omega\) into a collection of disjoint events \(B_1,\; B_2,\; \dots,\; B_n\),
    such that \(\bigcup_{i=1}^n B_i = \Omega\), we have
    \begin{equation*}
        \Pr{\left( A \right)} = \sum_{i = 1}^n \Pr{\left( A \,\vert\, B_i \right)}\Pr{\left( B_i \right)}
    \end{equation*}
\end{theorem}
\begin{theorem}[Bayes' Theorem]
    Given the probability for \(A\) given \(B\), the probability of the reverse direction is given by
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} = \frac{\Pr{\left( B \,\vert\, A \right)}\Pr{\left( A \right)}}{\Pr{\left( B \right)}}
    \end{equation*}
\end{theorem}
\section{Combinatorics}
\begin{definition}[Number of outcomes]
    Let \(\abs{A}\) denote the number of outcomes in an event \(A\).
\end{definition}
\begin{theorem}[Addition principle]
    Given a sample space \(S\) with \(k\) disjoint events \({\left\{ S_1,\:\ldots,\:S_k \right\}}\),
    where the \(i\)th event has \(n_i\) possible outcomes,
    the number of possible samples from any event is given by
    \begin{equation*}
        \abs{\bigcup_{i = 0}^{k} S_i} = \sum_{i = 1}^k n_i
    \end{equation*}
\end{theorem}
\begin{theorem}[Multiplication principle]
    Given a sample space \(S\) with \(k\) events \({\left\{ S_1,\:\ldots,\:S_k \right\}}\),
    where the \(i\)th event has \(n_i\) possible outcomes,
    the number of possible samples from every event is given by
    \begin{equation*}
        \abs{\bigcap_{i=0}^{k} S_i} = \prod_{i = 1}^k n_i
    \end{equation*}
\end{theorem}
\begin{theorem}[Counting probability]
    Given a sample space \(S\) with equally likely outcomes, the probability
    of an event \(S_i \subset S\) is given by
    \begin{equation*}
        \Pr{\left( S_i \right)} = \frac{\abs{S_i}}{\abs{S}}
    \end{equation*}
\end{theorem}
\subsection{Ordered Sampling with Replacement}
When ordering is important and repetition is allowed,
the total number of ways to choose \(k\) objects from a set with \(n\) elements is
\begin{equation*}
    n^k
\end{equation*}
\subsection{Ordered Sampling without Replacement}
When ordering is important and repetition is not allowed,
the total number of ways to arrange \(k\) objects from a set of \(n\) elements is
known as a \(k\)-permutation of \(n\)-elements denoted \(\prescript{n}{}{P}_k\)
\begin{align*}
    \prescript{n}{}{P}_k & = n \times \left( n - 1 \right) \times \cdots \times \left( n - k + 1 \right) \\
                         & = \frac{n!}{\left( n - k \right)!}
\end{align*}
for \(0 \leq k \leq n\).
\begin{definition}[Permutation of \(n\) elements]
    An \(n\)-permutation of \(n\) elements is the permutation of those elements.
    In this case, \(k = n\), so that
    \begin{align*}
        \prescript{n}{}{P}_n & = n \times \left( n - 1 \right) \times \cdots \times \left( n - n + 1 \right) \\
                             & = n!
    \end{align*}
\end{definition}
\subsection{Unordered Sampling without Replacement}
When ordering is not important and repetition is not allowed,
the total number of ways to choose \(k\) objects from a set of \(n\) elements is
known as a \(k\)-combination of \(n\)-elements denoted \(\prescript{n}{}{C}_k\) or \(\binom{n}{k}\)
\begin{align*}
    \prescript{n}{}{C}_k & = \frac{\prescript{n}{}{P}_k}{k!}     \\
                         & = \frac{n!}{k! \left( n - k \right)!}
\end{align*}
for \(0 \leq k \leq n\). We divide by \(k!\) because any \(k\)-element subset of \(n\)-elements % chktex 40
can be ordered in \(k!\) ways. % chktex 40
\subsection{Unordered Sampling with Replacement}
When ordering is not important and repetition is allowed,
the total number of ways to choose \(k\) objects from a set with \(n\) elements is
\begin{equation*}
    \binom{n + k - 1}{k}
\end{equation*}
\end{document}
